from datasets import load_dataset
import traceback
import json
import re
import numpy as np
import matplotlib.pyplot as plt
import sys
import signal
import ast
import astor
import multiprocessing
import pdb


class DummyOutput:
    def write(self, text):
        pass


def timeout_handler(signum, frame):
    raise TimeoutError("Execution timed out!")


# Set the timeout handler for the SIGALRM signal
signal.signal(signal.SIGALRM, timeout_handler)


def parser_code(model_response):
    split_model_output = model_response.split("```")
    if len(split_model_output) == 1:
        return False
    fixed_code = split_model_output[1].replace('python', '').replace('Python', '')
    return fixed_code


def execute_with_timeout(code, timeout_duration):
    try:
        # Set the alarm for the timeout duration
        signal.alarm(timeout_duration)

        # Execute the code
        exec(code, globals())
        while True:
            active_children = multiprocessing.active_children()
            if not active_children:
                break
            for process in active_children:
                process.join()
    finally:
        signal.alarm(0)


def test_humaneval_function(humaneval_entry: dict, model_output: str):
    """
    Tests a generated function from the HumanEval dataset.

    :param humaneval_entry: A dictionary containing 'prompt', 'entry_point', and 'test' keys from the HumanEval dataset.
    :param model_output: The code generated by the model as a string.
    :return: True if all test cases pass, False otherwise.
    """
    dummy_out = DummyOutput()
    # sys.stdout = dummy_out
    try:
        # Combine the prompt and model output to form the complete function

        # Define the function in the local scope
        execute_with_timeout(model_output, 5)
        execute_with_timeout(humaneval_entry['test'], 5)

        # Extract the entry point function name
        entry_point = humaneval_entry['entry_point']

        execute_with_timeout(f"check({entry_point})", 5)

        return True

    except Exception as e:
        print(f"An error occurred: {e}")
        print(traceback.format_exc())
        return False


def get_import_lines(code: str) -> str:
    # Split the code into lines
    lines = code.split('\n')
    # Filter lines that contain import statements
    import_lines = [line for line in lines if line.strip().startswith('import') or line.strip().startswith('from')]
    # Join the import lines back into a single string
    return '\n'.join(import_lines)


def find_first_non_indent_line(text, start_index):
    lines = text.splitlines()
    for i in range(start_index, len(lines)):
        # Check if the line does not start with a space or a tab
        if lines[i] and not lines[i].startswith((' ', '\t')):
            return i
    return -1  # Return -1 if no such line is found


def find_first_def_line(text, start_index):
    lines = text.splitlines()
    for i in range(start_index, len(lines)):
        if 'def ' in lines[i]:
            return i
    return -1


def extract_line_range(text, start_line, end_line):
    lines = text.splitlines()
    return '\n'.join(lines[start_line:end_line])


def extract_function(mixed_string, function_name):
    def_idx_fk = find_first_def_line(mixed_string, 0)
    if def_idx_fk == -1:
        return None
    end_idx_fk = find_first_non_indent_line(mixed_string, def_idx_fk+1)
    if end_idx_fk == -1:
        return None
    return extract_line_range(mixed_string, def_idx_fk, end_idx_fk)


def test_human_eval_dataset(all_generations, data_dict):

    results = dict()
    for coeff in all_generations:
        curr_generations = all_generations[coeff]
        success_perc_list = []
        for key in curr_generations:
            print(key)
            curr_generation_batch = curr_generations[key]
            curr_problem = data_dict[key]

            full_success_list = []
            for i in range(len(curr_generation_batch)):
                curr_answer = curr_generation_batch[i]
                curr_imports = get_import_lines(curr_answer)
                curr_function_code = extract_function(curr_answer + "\naaa", curr_problem['entry_point'])
                # if coeff == '0':
                #     pdb.set_trace()
                if curr_function_code is None:
                    continue
                curr_code_with_imports = f"{curr_imports}\n{curr_function_code}"
                is_pass = test_humaneval_function(curr_problem, curr_code_with_imports)
                full_success_list.append(is_pass)

            success_perc = np.sum(full_success_list) / len(curr_generation_batch)
            success_perc_list.append(success_perc)
        results[int(coeff)] = np.average(success_perc_list)
    return results


def plot_results(results):
    averages = [np.mean(results[key]) for key in results]
    stds = [np.std(results[key]) for key in results]
    keys_int = [int(key) for key in results]

    # Create the plot
    plt.figure(figsize=(10, 6))
    plt.errorbar(keys_int, averages, yerr=stds, fmt='o', capsize=5, capthick=2, ecolor='red',
                 markeredgecolor='black', markerfacecolor='blue')

    # Customize the plot
    plt.xlabel('coeff')
    plt.ylabel('mean success rate')
    plt.title('Success Rate vs. Coeff')
    plt.grid(True, linestyle='--', alpha=0.7)

    # Show the plot
    plt.tight_layout()
    plt.show()


def main():
    human_eval_data = load_dataset("openai/openai_humaneval")
    human_eval_dict = {q['task_id']: q for q in human_eval_data['test']}
    gen1 = open('code_generations_results_02_08_negative_coeff')
    gen1 = json.load(gen1)
    gen2 = open('code_generations_results_02_08_positive_coeff')
    gen2 = json.load(gen2)
    all_gen = gen1 | gen2

    results = test_human_eval_dataset(all_gen, human_eval_dict)
    print(results)

    plt.plot(results.keys(), results.values())
    plt.show()


if __name__ == '__main__':
    main()
