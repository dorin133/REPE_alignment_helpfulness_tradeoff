"""
HumanEval Code Execution and Testing Module

This module provides functionality for safely executing and testing code generated
by language models against the HumanEval dataset. It includes timeout protection
and comprehensive error handling to ensure safe execution of potentially unsafe code.
"""

import traceback
import signal
import sys
from typing import Dict, Any, Optional

# Common imports that are typically needed for HumanEval solutions
COMMON_IMPORTS = """
from numpy import concatenate, product, multiply
import json
import re
import numpy as np
import statistics
from statistics import mean
from typing import List, Optional, Tuple, Any
import statistics as stats
import hashlib
import math
import random
import itertools
import collections
from math import floor, ceil
"""

# Global timeout duration in seconds
DEFAULT_TIMEOUT = 5


def timeout_handler(signum: int, frame: Any) -> None:
    """
    Signal handler for timeout events during code execution.
    
    Args:
        signum: Signal number (SIGALRM)
        frame: Current stack frame
        
    Raises:
        TimeoutError: Always raised when timeout occurs
    """
    raise TimeoutError("Code execution timed out!")


# Register the timeout handler for the SIGALRM signal
signal.signal(signal.SIGALRM, timeout_handler)


def execute_with_timeout(code: str, timeout_duration: int, local_dict: Dict[str, Any]) -> None:
    """
    Execute Python code with a timeout to prevent infinite loops or long-running operations.
    
    Args:
        code: Python code string to execute
        timeout_duration: Maximum execution time in seconds
        local_dict: Local namespace dictionary for code execution
        
    Raises:
        TimeoutError: If execution exceeds timeout_duration
        Exception: Any exception raised during code execution
    """
    try:
        # Set the alarm for the timeout duration
        signal.alarm(timeout_duration)
        
        # Execute the code in the provided namespace
        exec(code, local_dict)
        
    finally:
        # Always clear the alarm, regardless of success or failure
        signal.alarm(0)


def validate_humaneval_entry(humaneval_entry: Dict[str, Any]) -> bool:
    """
    Validate that a HumanEval entry contains all required fields.
    
    Args:
        humaneval_entry: Dictionary containing HumanEval test case data
        
    Returns:
        bool: True if entry is valid, False otherwise
    """
    required_fields = ['prompt', 'entry_point', 'test']
    
    for field in required_fields:
        if field not in humaneval_entry:
            print(f"Error: Missing required field '{field}' in HumanEval entry")
            return False
        
        if not isinstance(humaneval_entry[field], str):
            print(f"Error: Field '{field}' must be a string")
            return False
    
    return True


def sanitize_model_output(model_output: str) -> str:
    """
    Sanitize and prepare model output for safe execution.
    
    Args:
        model_output: Raw code output from the language model
        
    Returns:
        str: Sanitized code string
    """
    # Remove any potential harmful operations (basic sanitization)
    # Note: This is not comprehensive security - use sandboxing for production
    
    # Strip whitespace and ensure proper formatting
    sanitized = model_output.strip()
    
    # Basic checks for potentially harmful operations
    dangerous_patterns = [
        'import os',
        'import subprocess',
        'import sys',
        '__import__',
        'eval(',
        'exec(',
        'open(',
        'file(',
    ]
    
    for pattern in dangerous_patterns:
        if pattern in sanitized.lower():
            print(f"Warning: Potentially dangerous pattern detected: {pattern}")
    
    return sanitized


def test_humaneval_function(humaneval_entry: Dict[str, Any], model_output: str, 
                           timeout_duration: int = DEFAULT_TIMEOUT, 
                           verbose: bool = False) -> bool:
    """
    Test a generated function against HumanEval test cases with timeout protection.

    Args:
        humaneval_entry: Dictionary containing 'prompt', 'entry_point', and 'test' keys
                        from the HumanEval dataset
        model_output: The code generated by the model as a string
        timeout_duration: Maximum execution time per operation in seconds
        verbose: Whether to print detailed error information
        
    Returns:
        bool: True if all test cases pass, False otherwise
    """
    # Validate input parameters
    if not validate_humaneval_entry(humaneval_entry):
        return False
    
    if not isinstance(model_output, str) or not model_output.strip():
        print("Error: Model output must be a non-empty string")
        return False
    
    try:
        # Create isolated namespace for code execution
        local_dict = {}
        
        # Sanitize the model output
        sanitized_output = sanitize_model_output(model_output)
        
        if verbose:
            print(f"Testing function: {humaneval_entry['entry_point']}")
            print(f"Model output length: {len(sanitized_output)} characters")
        
        # Step 1: Execute imports and model-generated function
        combined_code = COMMON_IMPORTS + "\n" + sanitized_output
        execute_with_timeout(combined_code, timeout_duration, local_dict)
        
        # Step 2: Execute the test cases
        execute_with_timeout(humaneval_entry['test'], timeout_duration, local_dict)
        
        # Step 3: Run the check function with the entry point
        entry_point = humaneval_entry['entry_point']
        check_command = f"check({entry_point})"
        execute_with_timeout(check_command, timeout_duration, local_dict)
        
        if verbose:
            print(f"All tests passed for function: {entry_point}")
        
        return True

    except TimeoutError as e:
        if verbose:
            print(f"Timeout error: {e}")
        return False
    
    except SyntaxError as e:
        if verbose:
            print(f"Syntax error in generated code: {e}")
            print(f"Line {e.lineno}: {e.text}")
        return False
    
    except NameError as e:
        if verbose:
            print(f"Name error (undefined variable/function): {e}")
        return False
    
    except Exception as e:
        if verbose:
            print(f"Test execution failed: {e}")
            print("Full traceback:")
            print(traceback.format_exc())
        return False


def batch_test_humaneval(test_cases: List[Dict[str, Any]], 
                        model_outputs: List[str],
                        timeout_duration: int = DEFAULT_TIMEOUT,
                        verbose: bool = False) -> Dict[str, Any]:
    """
    Test multiple HumanEval cases in batch and return detailed results.
    
    Args:
        test_cases: List of HumanEval entry dictionaries
        model_outputs: List of corresponding model-generated code strings
        timeout_duration: Maximum execution time per operation in seconds
        verbose: Whether to print detailed information during testing
        
    Returns:
        Dict containing test results and statistics
    """
    if len(test_cases) != len(model_outputs):
        raise ValueError("Number of test cases must match number of model outputs")
    
    results = {
        'total_tests': len(test_cases),
        'passed': 0,
        'failed': 0,
        'detailed_results': [],
        'pass_rate': 0.0
    }
    
    for i, (test_case, model_output) in enumerate(zip(test_cases, model_outputs)):
        if verbose:
            print(f"\nTesting case {i+1}/{len(test_cases)}")
        
        try:
            passed = test_humaneval_function(
                test_case, 
                model_output, 
                timeout_duration, 
                verbose
            )
            
            if passed:
                results['passed'] += 1
            else:
                results['failed'] += 1
            
            results['detailed_results'].append({
                'test_id': i,
                'entry_point': test_case.get('entry_point', 'unknown'),
                'passed': passed
            })
            
        except Exception as e:
            if verbose:
                print(f"Error processing test case {i}: {e}")
            results['failed'] += 1
            results['detailed_results'].append({
                'test_id': i,
                'entry_point': test_case.get('entry_point', 'unknown'),
                'passed': False,
                'error': str(e)
            })
    
    # Calculate pass rate
    if results['total_tests'] > 0:
        results['pass_rate'] = results['passed'] / results['total_tests']
    
    if verbose:
        print(f"\nBatch testing complete:")
        print(f"  Total tests: {results['total_tests']}")
        print(f"  Passed: {results['passed']}")
        print(f"  Failed: {results['failed']}")
        print(f"  Pass rate: {results['pass_rate']:.2%}")
    
    return results


# Example usage and testing
if __name__ == "__main__":
    # Example HumanEval entry for testing
    example_entry = {
        'prompt': 'def add_two_numbers(a, b):\n    """Add two numbers and return the result."""',
        'entry_point': 'add_two_numbers',
        'test': '''
def check(candidate):
    assert candidate(2, 3) == 5
    assert candidate(-1, 1) == 0
    assert candidate(0, 0) == 0
'''
    }
    
    # Example model output
    example_output = '''
def add_two_numbers(a, b):
    """Add two numbers and return the result."""
    return a + b
'''
    
    # Test the function
    result = test_humaneval_function(example_entry, example_output, verbose=True)
    print(f"Test result: {'PASS' if result else 'FAIL'}")